{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jonathan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import math\n",
    "import tensorflow_addons as tfa\n",
    "import keras_tuner as kt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"deep\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = cifar10\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the Swish activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape = [32,32,3]))\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100,\n",
    "                                    activation=\"swish\",\n",
    "                                    kernel_initializer=\"he_normal\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Ｕsing Nadam optimization and early stopping, train the network on the CIFAR10 dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有 10 個類，因此需要一個具有 10 個神經元的 softmax 輸出層。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add the output layer to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(10,activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nadam 優化是 Adam 優化加上 Nesterov 技巧，因此它往往會比 Adam 收斂得稍快一些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=optimizer,\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EarlyStopping callback and Tensorboard callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20,restore_best_weights=True)\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_cifar10_model\",save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = Path() / \"my_cifar10_logs\" / f\"run_{run_index:03d}\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在模型已準備好接受訓練。 為此，我們只需要調用它的 fit() 方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8,5), xlim=[0,59], ylim=[0,2.5], grid=True, xlabel=\"Epoch\",\n",
    "    style=[\"r--\", \"r--\", \"b-\", \"b-*\"])\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 12  # Total time in minutes\n",
    "num_epochs = 60  # Number of epochs\n",
    "\n",
    "time_per_epoch = total_time / num_epochs\n",
    "time_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_valid, y_valid, return_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "驗證集loss最低的模型在驗證集上的準確率約為 42.4%。 花了 28 個Epoch才達到最低驗證損失，在我的筆記本電腦上每個epoch大約需要 12 秒(0.2min)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_cifar10_model\", save_format=\"tf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 看看是否可以使用Batch Normalization來改進模型。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.  try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"my_cifar10_model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is very similar to the code above, with a few changes:\n",
    "\n",
    "* I added a BN layer after every Dense layer (before the activation function), except for the output layer.\n",
    "\n",
    "* I renamed the run directories to run_bn_* and the model file name to my_cifar10_bn_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation(\"swish\"))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20,\n",
    "                                                     restore_best_weights=True)\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model\",\n",
    "                                                         save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = Path() / \"my_cifar10_logs_BN\" / f\"run_{run_index:03d}\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8,5), xlim=[0,41], ylim=[0,2.5], grid=True, xlabel=\"Epoch\",\n",
    "    style=[\"r--\", \"r--\", \"b-\", \"b-*\"])\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the validation loss values from the history object\n",
    "val_losses = history.history['val_loss']\n",
    "\n",
    "# Get the validation accuracy values from the history object\n",
    "val_accs = history.history['val_accuracy']\n",
    "\n",
    "# Find the index of the epoch with the lowest validation loss\n",
    "best_epoch = np.argmin(val_losses)\n",
    "\n",
    "# Get the lowest validation loss and the corresponding epoch number\n",
    "lowest_val_loss = val_losses[best_epoch]\n",
    "lowest_val_acc = val_accs[best_epoch]\n",
    "epoch_with_lowest_loss = best_epoch + 1  # Add 1 since epochs are zero-indexed\n",
    "\n",
    "print(\"Epoch with the lowest validation loss:\", epoch_with_lowest_loss)\n",
    "print(\"Lowest validation loss:\", lowest_val_loss)\n",
    "print(\"Validation accuracy at the epoch:\", lowest_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 14  # Total time in minutes\n",
    "num_epochs = 40  # Number of epochs\n",
    "\n",
    "time_per_epoch = total_time / num_epochs\n",
    "time_per_epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The final model is also much better, with 50.7% validation accuracy\n",
    "\n",
    "* How does BN affect training speed? each epoch took about 21s instead of 12s, because of the extra computations required by the BN layers. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100,\n",
    "                                    kernel_initializer=\"lecun_normal\",\n",
    "                                    activation=\"selu\"))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=20, restore_best_weights=True)\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_cifar10_selu_model\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = Path() / \"my_cifar10_logs_selu\" / f\"run_selu_{run_index:03d}\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "#輸入特徵必須標準化：均值 0 和標準差 1。\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8,5), xlim=[0,33], ylim=[0,2.5], grid=True, xlabel=\"Epoch\",\n",
    "    style=[\"r--\", \"r--\", \"b-\", \"b-*\"])\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the validation loss values from the history object\n",
    "val_losses = history.history['val_loss']\n",
    "\n",
    "# Get the validation accuracy values from the history object\n",
    "val_accs = history.history['val_accuracy']\n",
    "\n",
    "# Find the index of the epoch with the lowest validation loss\n",
    "best_epoch = np.argmin(val_losses)\n",
    "\n",
    "# Get the lowest validation loss and the corresponding epoch number\n",
    "lowest_val_loss = val_losses[best_epoch]\n",
    "lowest_val_acc = val_accs[best_epoch]\n",
    "epoch_with_lowest_loss = best_epoch + 1  # Add 1 since epochs are zero-indexed\n",
    "\n",
    "print(\"Epoch with the lowest validation loss:\", epoch_with_lowest_loss)\n",
    "print(\"Lowest validation loss:\", lowest_val_loss)\n",
    "print(\"Validation accuracy at the epoch:\", lowest_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 6.5  # Total time in minutes\n",
    "num_epochs = 34  # Number of epochs\n",
    "\n",
    "time_per_epoch = total_time / num_epochs\n",
    "time_per_epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model reached reached its lowest validation loss, with about 49.2% accuracy, which is better than the original model(42.4%), close to using batch normalization(49.5%). Each epoch took only 11 seconds. So it's the fastest model to train so far."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100,\n",
    "                                    kernel_initializer=\"lecun_normal\",\n",
    "                                    activation=\"selu\"))\n",
    "\n",
    "model.add(tf.keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=20, restore_best_weights=True)\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_cifar10_alpha_dropout_model\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = Path() / \"my_cifar10_logs_alpha_dropout\" / f\"run_alpha_dropout_{run_index:03d}\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8,5), xlim=[0,27], ylim=[0,2.5], grid=True, xlabel=\"Epoch\",\n",
    "    style=[\"r--\", \"r--\", \"b-\", \"b-*\"])\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the validation loss values from the history object\n",
    "val_losses = history.history['val_loss']\n",
    "\n",
    "# Get the validation accuracy values from the history object\n",
    "val_accs = history.history['val_accuracy']\n",
    "\n",
    "# Find the index of the epoch with the lowest validation loss\n",
    "best_epoch = np.argmin(val_losses)\n",
    "\n",
    "# Get the lowest validation loss and the corresponding epoch number\n",
    "lowest_val_loss = val_losses[best_epoch]\n",
    "lowest_val_acc = val_accs[best_epoch]\n",
    "epoch_with_lowest_loss = best_epoch + 1  # Add 1 since epochs are zero-indexed\n",
    "\n",
    "print(\"Epoch with the lowest validation loss:\", epoch_with_lowest_loss)\n",
    "print(\"Lowest validation loss:\", lowest_val_loss)\n",
    "print(\"Validation accuracy at the epoch:\", lowest_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 5  # Total time in minutes\n",
    "num_epochs = 28  # Number of epochs\n",
    "\n",
    "time_per_epoch = total_time / num_epochs\n",
    "time_per_epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model reaches 46.2% accuracy on the validation set. That's worse than without dropout (49.2%). With an extensive hyperparameter search, it might be possible to do better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use MC Dropout now. We will need the MCAlphaDropout class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(tf.keras.layers.AlphaDropout):\n",
    "    def call(self, inputs,training=None):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new model, identical to the one we just trained (with the same weights), but with MCAlphaDropout dropout layers instead of AlphaDropout layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout = tf.keras.layers.Dropout\n",
    "mc_model = tf.keras.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, Dropout) else layer\n",
    "    for layer in model.layers\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's add a couple utility functions. The first will run the model many times (10 by default) and it will return the mean predicted class probabilities. The second will use these mean probabilities to predict the most likely class for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return Y_probas.argmax(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make predictions for all the instances in the validation set, and compute the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = (y_pred == y_valid[:, 0]).mean()\n",
    "accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So the best model we got in this exercise is the Batch Normalization model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我自己來調參做做看"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1cycle scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 25\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEARNING_RATE = 0.01\n",
    "START_LEARNING_RATE = 0.001\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "CYCLICAL_LEARNING_RATE = tfa.optimizers.CyclicalLearningRate(\n",
    "    initial_learning_rate=START_LEARNING_RATE,\n",
    "    maximal_learning_rate=MAX_LEARNING_RATE,\n",
    "    scale_fn=lambda x: 1/(2.**(x-1)),\n",
    "    step_size=2 * steps_per_epoch\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value = 15, max_value = 25, default = 20)\n",
    "    n_neurons = hp.Int(\"n_neurons\", min_value = 50, max_value = 270)\n",
    "    optimizer = hp.Choice(\"optimizer\", values = [\"sgd\", \"nadam\", \"adam\", \"adamw\"])\n",
    "    if optimizer == \"sge\":\n",
    "        optimizer = tf.keras.optimizers.SGD(momentum=0.9, nesterov=True, learning_rate=CYCLICAL_LEARNING_RATE)\n",
    "    elif  optimizer == \"nadam\":\n",
    "        optimizer = tf.keras.optimizers.Nadam(beta_1=0.9, beta_2=0.999, learning_rate=CYCLICAL_LEARNING_RATE)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.999, learning_rate=CYCLICAL_LEARNING_RATE)\n",
    "    elif optimizer == \"adamw\":\n",
    "        optimizer = tfa.optimizers.AdamW(weight_decay=1e-5, beta_1=0.9, beta_2=0.999, learning_rate=CYCLICAL_LEARNING_RATE)\n",
    "    activation_function = hp.Choice(\"activation_function\", values = [\"elu\", \"relu\", \"swish\"])\n",
    "    \n",
    "    my_model = tf.keras.Sequential()\n",
    "    my_model.add(tf.keras.layers.Flatten(input_shape = [32,32,3]))\n",
    "    for _ in range(n_hidden):\n",
    "        my_model.add(tf.keras.layers.Dense(n_neurons, kernel_initializer=\"he_normal\"))\n",
    "        my_model.add(tf.keras.layers.BatchNormalization())\n",
    "        my_model.add(tf.keras.layers.Activation(activation=activation_function,))\n",
    "    my_model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    my_model.compile(loss = \"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    \n",
    "    return my_model\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassificationHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        return build_model(hp)\n",
    "    \n",
    "    def fit(self, hp, model, X, y, **kwargs):\n",
    "        if hp.Boolean(\"normalize\"):\n",
    "            norm_layer = tf.keras.layers.Normalization()\n",
    "            X = norm_layer(X)\n",
    "        return model.fit(X, y, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_opt_tuner = kt.BayesianOptimization(\n",
    "    MyClassificationHyperModel(), objective=\"val_accuracy\", seed=42,\n",
    "    max_trials=10, alpha=1e-4, beta=2.6,\n",
    "    overwrite = True, directory = \"my_cifar10_best\", project_name = \"bayesian_opt\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#EarlyStopping\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=20, restore_best_weights=True)\n",
    "#ModelCheckpoint\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_best_model\", save_best_only=True)\n",
    "#TensorBoard\n",
    "root_logdir = Path(bayesian_opt_tuner.project_dir)/\"tensorboard\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(root_logdir)\n",
    "\n",
    "\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 25 epochs to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 09m 39s]\n",
      "val_accuracy: 0.527400016784668\n",
      "\n",
      "Best val_accuracy So Far: 0.5465999841690063\n",
      "Total elapsed time: 01h 56m 49s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "bayesian_opt_tuner.search(X_train, y_train, epochs = 25, \n",
    "                          validation_data = (X_valid, y_valid), \n",
    "                          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.sequential.Sequential at 0x197b5323e90>,\n",
       " <keras.engine.sequential.Sequential at 0x197b4e977d0>,\n",
       " <keras.engine.sequential.Sequential at 0x197f0c9b290>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_opt_tuner.get_best_models(num_models=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 17\n",
      "n_neurons: 266\n",
      "optimizer: adam\n",
      "activation_function: swish\n",
      "normalize: False\n",
      "Score: 0.5465999841690063\n"
     ]
    }
   ],
   "source": [
    "best_trial = bayesian_opt_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "best_trial.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5465999841690063"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial.metrics.get_last_value(\"val_accuracy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logdir = Path(\"C:/Users/Jonathan/Documents/BA/Python Data Science/Sklearn and TensorFlow ML Book/Deep learning/11. Training Deep Neural Networks/my_cifar10_best/bayesian_opt/tensorboard\")\n",
    "writer = tf.summary.create_file_writer(str(test_logdir))\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000 + 1):\n",
    "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n",
    "        \n",
    "        data = (np.random.randn(100) + 2) * step / 100  # gets larger\n",
    "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
    "        \n",
    "        images = np.random.rand(2, 32, 32, 3) * step / 1000  # gets brighter\n",
    "        tf.summary.image(\"my_images\", images, step=step)\n",
    "        \n",
    "        texts = [\"The step is \" + str(step), \"Its square is \" + str(step ** 2)]\n",
    "        tf.summary.text(\"my_text\", texts, step=step)\n",
    "        \n",
    "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 8232), started 0:06:06 ago. (Use '!kill 8232' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6b790a786bac32fc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6b790a786bac32fc\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=\"C:/Users/Jonathan/Documents/BA/Python Data Science/Sklearn and TensorFlow ML Book/Deep learning/11. Training Deep Neural Networks/my_cifar10_best/bayesian_opt/tensorboard\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue training on best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model  = bayesian_opt_tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1563/1563 [==============================] - 67s 36ms/step - loss: 0.9560 - accuracy: 0.6663\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.9412 - accuracy: 0.6692\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 64s 41ms/step - loss: 0.9115 - accuracy: 0.6801\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 65s 42ms/step - loss: 0.8776 - accuracy: 0.6912\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 63s 40ms/step - loss: 0.8581 - accuracy: 0.6964\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 60s 39ms/step - loss: 0.8368 - accuracy: 0.7038\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 58s 37ms/step - loss: 0.8100 - accuracy: 0.7149\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 58s 37ms/step - loss: 0.7851 - accuracy: 0.7217\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.7723 - accuracy: 0.7261\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 59s 38ms/step - loss: 0.7521 - accuracy: 0.7326\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 62s 40ms/step - loss: 0.7378 - accuracy: 0.7369\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 58s 37ms/step - loss: 0.7144 - accuracy: 0.7464\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 57s 36ms/step - loss: 0.7070 - accuracy: 0.7467\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 61s 39ms/step - loss: 0.6894 - accuracy: 0.7553\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 62s 40ms/step - loss: 0.6762 - accuracy: 0.7602\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6582 - accuracy: 0.7653\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.6488 - accuracy: 0.7689\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 85s 54ms/step - loss: 0.6308 - accuracy: 0.7749\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 66s 42ms/step - loss: 0.6237 - accuracy: 0.7779\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 61s 39ms/step - loss: 0.6141 - accuracy: 0.7806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x197e21cd050>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fit(X_train_full, y_train_full, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 1.8035 - accuracy: 0.5173\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
